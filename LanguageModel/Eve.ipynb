{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import CHILDESCorpusReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "corpus = nltk.data.find('/Users/Ershui13/nltk_data/corpora/childescorpus/')\n",
    "Eve = CHILDESCorpusReader(corpus, 'Brown/Eve/.*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pronounlist=['i', 'me', 'you', 'he' ,'him', 'she', 'her', 'we', 'us', 'they', 'them', 'it']\n",
    "Nom = ['i', 'he', 'she', 'we', 'they']\n",
    "Acc = ['me', 'him', 'her', 'us', 'them']\n",
    "mot_sent = Eve.sents(speaker = ['MOT'])\n",
    "Eve_sent = Eve.sents(speaker = ['CHI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgram (ngram):\n",
    "    p_gram = []\n",
    "    for gr in ngram:\n",
    "        for p in pronounlist:\n",
    "            if p in gr:\n",
    "                p_gram.append(gr)\n",
    "    return p_gram\n",
    "def save(t,l):\n",
    "    df = pd.DataFrame(t)\n",
    "    df.to_csv(l,sep=',',index=False)\n",
    "\n",
    "def ngram (n,sents):\n",
    "    tempgram = []\n",
    "    for s in sents:\n",
    "        ss = ' '.join(s) \n",
    "        ss = ss.lower()\n",
    "        ss = '<s> ' + ss + ' <\\s>'\n",
    "        tokens = [token for token in ss.split(' ') if token != '']\n",
    "        output = list(ngrams(tokens, n))\n",
    "        tempgram.append(output)\n",
    "        gram = list(itertools.chain(*tempgram))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Eve_3gram = ngram(3,Eve_sent)\n",
    "Eve_mot_3gram = ngram(3, mot_sent)\n",
    "Eve_p_3gram = pgram(Eve_3gram)\n",
    "Eve_mot_p_3gram = pgram(Eve_mot_3gram)\n",
    "Eve_2gram = ngram(2,Eve_sent )\n",
    "Eve_mot_2gram = ngram(2, mot_sent)\n",
    "Eve_p_2gram = pgram(Eve_2gram)\n",
    "Eve_mot_p_2gram = pgram(Eve_mot_2gram)\n",
    "save(Eve_p_3gram, 'Eve_p_3gram.csv')\n",
    "save(Eve_mot_p_3gram, 'Eve_mot_p_3gram.csv')\n",
    "save(Eve_2gram, 'Eve_2gram.csv')\n",
    "save(Eve_mot_2gram,'Eve_mot_2gram.csv')\n",
    "save(Eve_p_2gram,'Eve_p_2gram.csv')\n",
    "save(Eve_mot_p_2gram,'Eve_mot_p_2gram.csv')\n",
    "save(Eve_3gram, 'Eve_3gram.csv')\n",
    "save(Eve_mot_3gram, 'Eve_mot_3gram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52988"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Eve_mot_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Eve_mot_aX_Nom = [pair for pair in Eve_mot_p_2gram if pair[1] in Nom]\n",
    "Eve_mot_aX_Acc = [pair for pair in Eve_mot_p_2gram if pair[1] in Acc]\n",
    "Eve_mot_Xb_Nom = [pair for pair in Eve_mot_p_2gram if pair[0] in Nom]\n",
    "Eve_mot_Xb_Acc = [pair for pair in Eve_mot_p_2gram if pair[0] in Acc]\n",
    "Eve_mot_aXNX = [(pair[0], 'X') for pair in Eve_mot_aX_Nom]\n",
    "Eve_mot_aXAX = [(pair[0], 'X') for pair in Eve_mot_aX_Acc]\n",
    "Eve_mot_XbNX = [('X', pair[1]) for pair in Eve_mot_Xb_Nom]\n",
    "Eve_mot_XbAX = [('X', pair[1]) for pair in Eve_mot_Xb_Acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221 206\n"
     ]
    }
   ],
   "source": [
    "print(len(Eve_mot_aX_Nom), len(set(Eve_mot_aX_Nom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525 166\n"
     ]
    }
   ],
   "source": [
    "print(len(Eve_mot_aX_Acc), len(set(Eve_mot_aX_Acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217 325\n"
     ]
    }
   ],
   "source": [
    "print(len(Eve_mot_Xb_Nom), len(set(Eve_mot_Xb_Nom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527 170\n"
     ]
    }
   ],
   "source": [
    "print(len(Eve_mot_Xb_Acc), len(set(Eve_mot_Xb_Acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 629\n"
     ]
    }
   ],
   "source": [
    "print(len(set(Eve_mot_aXNX)-set(Eve_mot_aXAX)), sum([FreqDist(Eve_mot_aXNX)[u] for u in (set(Eve_mot_aXNX) - set(Eve_mot_aXAX))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 463\n"
     ]
    }
   ],
   "source": [
    "print(len(set(Eve_mot_aXAX)-set(Eve_mot_aXNX)), sum([FreqDist(Eve_mot_aXAX)[u] for u in (set(Eve_mot_aXAX) - set(Eve_mot_aXNX))]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 299\n"
     ]
    }
   ],
   "source": [
    "print(len(set(Eve_mot_XbAX)-set(Eve_mot_XbNX)), sum([FreqDist(Eve_mot_XbAX)[u] for u in (set(Eve_mot_XbAX) - set(Eve_mot_XbNX))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 936\n"
     ]
    }
   ],
   "source": [
    "print(len(set(Eve_mot_XbNX)-set(Eve_mot_XbAX)), sum([FreqDist(Eve_mot_XbNX)[u] for u in (set(Eve_mot_XbNX) - set(Eve_mot_XbAX))]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Eve_mot_aXb_Nom = [p for p in Eve_mot_p_3gram if p[1] in Nom]\n",
    "Eve_mot_aXb_Acc = [p for p in Eve_mot_p_3gram if p[1] in Acc]\n",
    "Eve_mot_aNXb = [(pair[0],'X',pair[2]) for pair in Eve_mot_aXb_Nom]\n",
    "Eve_mot_aAXb = [(pair[0],'X',pair[2]) for pair in Eve_mot_aXb_Acc]\n",
    "Eve_aXb_Nom = [p for p in Eve_p_3gram if p[1] in Nom]\n",
    "Eve_aXb_Acc = [p for p in Eve_p_3gram if p[1] in Acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221 614\n"
     ]
    }
   ],
   "source": [
    "print(len(Eve_mot_aXb_Nom),len(set(Eve_mot_aXb_Nom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527 299\n"
     ]
    }
   ],
   "source": [
    "print(len(Eve_mot_aXb_Acc),len(set(Eve_mot_aXb_Acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ershui13/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfnn_aXb_Nom = DataFrame(Eve_mot_aXb_Nom)\n",
    "dfnn_aXb_Acc = DataFrame(Eve_mot_aXb_Acc)\n",
    "dfnn_aXb_Nom.columns = ['a', 'X', 'b']\n",
    "dfnn_aXb_Acc.columns = ['a', 'X', 'b']\n",
    "dfnn_aXb_Nom['case'] = 'NOM'\n",
    "dfnn_aXb_Acc['case'] = 'ACC'\n",
    "dfnn_aXb_Nom['predictor'] = dfnn_aXb_Nom[['a', 'b']].apply(lambda x: '_'.join(x), axis=1)\n",
    "dfnn_aXb_Acc['predictor'] = dfnn_aXb_Acc[['a', 'b']].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "dfnn_Eve_Nom = DataFrame(Eve_aXb_Nom)\n",
    "dfnn_Eve_Acc = DataFrame(Eve_aXb_Acc)\n",
    "dfnn_Eve_Nom.columns = ['a', 'X', 'b']\n",
    "dfnn_Eve_Acc.columns = ['a', 'X', 'b']\n",
    "dfnn_Eve_Nom['case'] = 'NOM'\n",
    "dfnn_Eve_Acc['case'] = 'ACC'\n",
    "dfnn_Eve_Nom['predictor'] = dfnn_Eve_Nom[['a', 'b']].apply(lambda x: '_'.join(x), axis=1)\n",
    "dfnn_Eve_Acc['predictor'] = dfnn_Eve_Acc[['a', 'b']].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "dfnn_Eve = dfnn_Eve_Nom.append(dfnn_Eve_Acc)\n",
    "\n",
    "dfnn_Eve['aX'] = dfnn_Eve['a'] + '_'\n",
    "dfnn_Eve['Xb'] =  '_' + dfnn_Eve['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>X</th>\n",
       "      <th>b</th>\n",
       "      <th>case</th>\n",
       "      <th>predictor</th>\n",
       "      <th>aX</th>\n",
       "      <th>Xb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>did</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_did</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find</td>\n",
       "      <td>i</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>NOM</td>\n",
       "      <td>find_&lt;\\s&gt;</td>\n",
       "      <td>find_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>fall</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_fall</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>dance</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_dance</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>see</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_see</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxx</td>\n",
       "      <td>i</td>\n",
       "      <td>cider</td>\n",
       "      <td>NOM</td>\n",
       "      <td>xxx_cider</td>\n",
       "      <td>xxx_</td>\n",
       "      <td>_cider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>kick</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_kick</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>can</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_can</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>was</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_was</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_want</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>see</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_see</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>fell</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_fell</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_fell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>get</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_get</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>see</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_see</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>help</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_help</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>help</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_help</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>xxx</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_xxx</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>get</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_get</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>get</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_get</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>drop</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_drop</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>drop</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_drop</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>write</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_write</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_write</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>see</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_see</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>sock</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_sock</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_sock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>see</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_see</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>crack</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_crack</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_want</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>dance</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_dance</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>xxx</td>\n",
       "      <td>i</td>\n",
       "      <td>banging</td>\n",
       "      <td>NOM</td>\n",
       "      <td>xxx_banging</td>\n",
       "      <td>xxx_</td>\n",
       "      <td>_banging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>banging</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_banging</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_banging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>her</td>\n",
       "      <td>crying</td>\n",
       "      <td>ACC</td>\n",
       "      <td>&lt;s&gt;_crying</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_crying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>w</td>\n",
       "      <td>her</td>\n",
       "      <td>head</td>\n",
       "      <td>ACC</td>\n",
       "      <td>w_head</td>\n",
       "      <td>w_</td>\n",
       "      <td>_head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>there</td>\n",
       "      <td>us</td>\n",
       "      <td>is</td>\n",
       "      <td>ACC</td>\n",
       "      <td>there_is</td>\n",
       "      <td>there_</td>\n",
       "      <td>_is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>help</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>help_&lt;\\s&gt;</td>\n",
       "      <td>help_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>write</td>\n",
       "      <td>her</td>\n",
       "      <td>head</td>\n",
       "      <td>ACC</td>\n",
       "      <td>write_head</td>\n",
       "      <td>write_</td>\n",
       "      <td>_head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>for</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>for_&lt;\\s&gt;</td>\n",
       "      <td>for_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>at</td>\n",
       "      <td>her</td>\n",
       "      <td>sarah</td>\n",
       "      <td>ACC</td>\n",
       "      <td>at_sarah</td>\n",
       "      <td>at_</td>\n",
       "      <td>_sarah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>at</td>\n",
       "      <td>her</td>\n",
       "      <td>sarah</td>\n",
       "      <td>ACC</td>\n",
       "      <td>at_sarah</td>\n",
       "      <td>at_</td>\n",
       "      <td>_sarah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>at</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>at_&lt;\\s&gt;</td>\n",
       "      <td>at_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>at</td>\n",
       "      <td>her</td>\n",
       "      <td>xxx</td>\n",
       "      <td>ACC</td>\n",
       "      <td>at_xxx</td>\n",
       "      <td>at_</td>\n",
       "      <td>_xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>her</td>\n",
       "      <td>didn't</td>\n",
       "      <td>ACC</td>\n",
       "      <td>&lt;s&gt;_didn't</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_didn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>her</td>\n",
       "      <td>baby</td>\n",
       "      <td>ACC</td>\n",
       "      <td>&lt;s&gt;_baby</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>xxx</td>\n",
       "      <td>her</td>\n",
       "      <td>key</td>\n",
       "      <td>ACC</td>\n",
       "      <td>xxx_key</td>\n",
       "      <td>xxx_</td>\n",
       "      <td>_key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>with</td>\n",
       "      <td>her</td>\n",
       "      <td>key</td>\n",
       "      <td>ACC</td>\n",
       "      <td>with_key</td>\n",
       "      <td>with_</td>\n",
       "      <td>_key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>to</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>to_&lt;\\s&gt;</td>\n",
       "      <td>to_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>to</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>to_&lt;\\s&gt;</td>\n",
       "      <td>to_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>help</td>\n",
       "      <td>us</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>help_&lt;\\s&gt;</td>\n",
       "      <td>help_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>for</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>for_&lt;\\s&gt;</td>\n",
       "      <td>for_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>with</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>with_&lt;\\s&gt;</td>\n",
       "      <td>with_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_&lt;\\s&gt;</td>\n",
       "      <td>see_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_&lt;\\s&gt;</td>\n",
       "      <td>see_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>her</td>\n",
       "      <td>to</td>\n",
       "      <td>ACC</td>\n",
       "      <td>&lt;s&gt;_to</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>let</td>\n",
       "      <td>me</td>\n",
       "      <td>see</td>\n",
       "      <td>ACC</td>\n",
       "      <td>let_see</td>\n",
       "      <td>let_</td>\n",
       "      <td>_see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>dem</td>\n",
       "      <td>them</td>\n",
       "      <td>are</td>\n",
       "      <td>ACC</td>\n",
       "      <td>dem_are</td>\n",
       "      <td>dem_</td>\n",
       "      <td>_are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>brought</td>\n",
       "      <td>me</td>\n",
       "      <td>some</td>\n",
       "      <td>ACC</td>\n",
       "      <td>brought_some</td>\n",
       "      <td>brought_</td>\n",
       "      <td>_some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>to</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>to_&lt;\\s&gt;</td>\n",
       "      <td>to_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>picking</td>\n",
       "      <td>her</td>\n",
       "      <td>up</td>\n",
       "      <td>ACC</td>\n",
       "      <td>picking_up</td>\n",
       "      <td>picking_</td>\n",
       "      <td>_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_&lt;\\s&gt;</td>\n",
       "      <td>see_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_&lt;\\s&gt;</td>\n",
       "      <td>see_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>ACC</td>\n",
       "      <td>&lt;s&gt;_be</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_be</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2114 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a     X        b case     predictor        aX        Xb\n",
       "0        <s>     i      did  NOM       <s>_did      <s>_      _did\n",
       "1       find     i     <\\s>  NOM     find_<\\s>     find_     _<\\s>\n",
       "2        <s>     i     fall  NOM      <s>_fall      <s>_     _fall\n",
       "3        <s>     i    dance  NOM     <s>_dance      <s>_    _dance\n",
       "4        <s>     i      see  NOM       <s>_see      <s>_      _see\n",
       "5        xxx     i    cider  NOM     xxx_cider      xxx_    _cider\n",
       "6        <s>     i     kick  NOM      <s>_kick      <s>_     _kick\n",
       "7        <s>     i      can  NOM       <s>_can      <s>_      _can\n",
       "8        <s>     i      was  NOM       <s>_was      <s>_      _was\n",
       "9        <s>     i     want  NOM      <s>_want      <s>_     _want\n",
       "10       <s>     i      see  NOM       <s>_see      <s>_      _see\n",
       "11       <s>     i     fell  NOM      <s>_fell      <s>_     _fell\n",
       "12       <s>     i      get  NOM       <s>_get      <s>_      _get\n",
       "13       <s>     i      see  NOM       <s>_see      <s>_      _see\n",
       "14       <s>     i     help  NOM      <s>_help      <s>_     _help\n",
       "15       <s>     i     help  NOM      <s>_help      <s>_     _help\n",
       "16       <s>     i      xxx  NOM       <s>_xxx      <s>_      _xxx\n",
       "17       <s>     i      get  NOM       <s>_get      <s>_      _get\n",
       "18       <s>     i      get  NOM       <s>_get      <s>_      _get\n",
       "19       <s>     i     drop  NOM      <s>_drop      <s>_     _drop\n",
       "20       <s>     i     drop  NOM      <s>_drop      <s>_     _drop\n",
       "21       <s>     i    write  NOM     <s>_write      <s>_    _write\n",
       "22       <s>     i      see  NOM       <s>_see      <s>_      _see\n",
       "23       <s>     i     sock  NOM      <s>_sock      <s>_     _sock\n",
       "24       <s>     i      see  NOM       <s>_see      <s>_      _see\n",
       "25       <s>     i    crack  NOM     <s>_crack      <s>_    _crack\n",
       "26       <s>     i     want  NOM      <s>_want      <s>_     _want\n",
       "27       <s>     i    dance  NOM     <s>_dance      <s>_    _dance\n",
       "28       xxx     i  banging  NOM   xxx_banging      xxx_  _banging\n",
       "29       <s>     i  banging  NOM   <s>_banging      <s>_  _banging\n",
       "..       ...   ...      ...  ...           ...       ...       ...\n",
       "406      <s>   her   crying  ACC    <s>_crying      <s>_   _crying\n",
       "407        w   her     head  ACC        w_head        w_     _head\n",
       "408    there    us       is  ACC      there_is    there_       _is\n",
       "409     help    me     <\\s>  ACC     help_<\\s>     help_     _<\\s>\n",
       "410    write   her     head  ACC    write_head    write_     _head\n",
       "411      for   her     <\\s>  ACC      for_<\\s>      for_     _<\\s>\n",
       "412       at   her    sarah  ACC      at_sarah       at_    _sarah\n",
       "413       at   her    sarah  ACC      at_sarah       at_    _sarah\n",
       "414       at   her     <\\s>  ACC       at_<\\s>       at_     _<\\s>\n",
       "415       at   her      xxx  ACC        at_xxx       at_      _xxx\n",
       "416      <s>   her   didn't  ACC    <s>_didn't      <s>_   _didn't\n",
       "417      <s>   her     baby  ACC      <s>_baby      <s>_     _baby\n",
       "418      xxx   her      key  ACC       xxx_key      xxx_      _key\n",
       "419     with   her      key  ACC      with_key     with_      _key\n",
       "420       to    me     <\\s>  ACC       to_<\\s>       to_     _<\\s>\n",
       "421       to    me     <\\s>  ACC       to_<\\s>       to_     _<\\s>\n",
       "422     help    us     <\\s>  ACC     help_<\\s>     help_     _<\\s>\n",
       "423      for    me     <\\s>  ACC      for_<\\s>      for_     _<\\s>\n",
       "424     with    me     <\\s>  ACC     with_<\\s>     with_     _<\\s>\n",
       "425      see   her     <\\s>  ACC      see_<\\s>      see_     _<\\s>\n",
       "426      see   her     <\\s>  ACC      see_<\\s>      see_     _<\\s>\n",
       "427      <s>   her       to  ACC        <s>_to      <s>_       _to\n",
       "428      let    me      see  ACC       let_see      let_      _see\n",
       "429      dem  them      are  ACC       dem_are      dem_      _are\n",
       "430  brought    me     some  ACC  brought_some  brought_     _some\n",
       "431       to   her     <\\s>  ACC       to_<\\s>       to_     _<\\s>\n",
       "432  picking   her       up  ACC    picking_up  picking_       _up\n",
       "433      see   her     <\\s>  ACC      see_<\\s>      see_     _<\\s>\n",
       "434      see   her     <\\s>  ACC      see_<\\s>      see_     _<\\s>\n",
       "435      <s>   her       be  ACC        <s>_be      <s>_       _be\n",
       "\n",
       "[2114 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnn_Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>X</th>\n",
       "      <th>b</th>\n",
       "      <th>case</th>\n",
       "      <th>predictor</th>\n",
       "      <th>aX</th>\n",
       "      <th>Xb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh</td>\n",
       "      <td>i</td>\n",
       "      <td>took</td>\n",
       "      <td>NOM</td>\n",
       "      <td>oh_took</td>\n",
       "      <td>oh_</td>\n",
       "      <td>_took</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>think</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_think</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>he</td>\n",
       "      <td>gave</td>\n",
       "      <td>NOM</td>\n",
       "      <td>yes_gave</td>\n",
       "      <td>yes_</td>\n",
       "      <td>_gave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>just</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_just</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thought</td>\n",
       "      <td>she</td>\n",
       "      <td>might</td>\n",
       "      <td>NOM</td>\n",
       "      <td>thought_might</td>\n",
       "      <td>thought_</td>\n",
       "      <td>_might</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no</td>\n",
       "      <td>i</td>\n",
       "      <td>don't</td>\n",
       "      <td>NOM</td>\n",
       "      <td>no_don't</td>\n",
       "      <td>no_</td>\n",
       "      <td>_don't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>don't</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_don't</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_don't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>think</td>\n",
       "      <td>she</td>\n",
       "      <td>would</td>\n",
       "      <td>NOM</td>\n",
       "      <td>think_would</td>\n",
       "      <td>think_</td>\n",
       "      <td>_would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if</td>\n",
       "      <td>we</td>\n",
       "      <td>can</td>\n",
       "      <td>NOM</td>\n",
       "      <td>if_can</td>\n",
       "      <td>if_</td>\n",
       "      <td>_can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>he</td>\n",
       "      <td>doesn't</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_doesn't</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_doesn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>would</td>\n",
       "      <td>he</td>\n",
       "      <td>like</td>\n",
       "      <td>NOM</td>\n",
       "      <td>would_like</td>\n",
       "      <td>would_</td>\n",
       "      <td>_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>he</td>\n",
       "      <td>would</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_would</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>if</td>\n",
       "      <td>he</td>\n",
       "      <td>would</td>\n",
       "      <td>NOM</td>\n",
       "      <td>if_would</td>\n",
       "      <td>if_</td>\n",
       "      <td>_would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>he</td>\n",
       "      <td>doesn't</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_doesn't</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_doesn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>he</td>\n",
       "      <td>wanted</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_wanted</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_wanted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>he</td>\n",
       "      <td>did</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_did</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>don't</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_don't</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_don't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>do</td>\n",
       "      <td>we</td>\n",
       "      <td>have_to</td>\n",
       "      <td>NOM</td>\n",
       "      <td>do_have_to</td>\n",
       "      <td>do_</td>\n",
       "      <td>_have_to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>does</td>\n",
       "      <td>she</td>\n",
       "      <td>like</td>\n",
       "      <td>NOM</td>\n",
       "      <td>does_like</td>\n",
       "      <td>does_</td>\n",
       "      <td>_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>well</td>\n",
       "      <td>we</td>\n",
       "      <td>can't</td>\n",
       "      <td>NOM</td>\n",
       "      <td>well_can't</td>\n",
       "      <td>well_</td>\n",
       "      <td>_can't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>if</td>\n",
       "      <td>we</td>\n",
       "      <td>can</td>\n",
       "      <td>NOM</td>\n",
       "      <td>if_can</td>\n",
       "      <td>if_</td>\n",
       "      <td>_can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>if</td>\n",
       "      <td>i</td>\n",
       "      <td>can</td>\n",
       "      <td>NOM</td>\n",
       "      <td>if_can</td>\n",
       "      <td>if_</td>\n",
       "      <td>_can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pencil</td>\n",
       "      <td>i</td>\n",
       "      <td>know</td>\n",
       "      <td>NOM</td>\n",
       "      <td>pencil_know</td>\n",
       "      <td>pencil_</td>\n",
       "      <td>_know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>there</td>\n",
       "      <td>she</td>\n",
       "      <td>is</td>\n",
       "      <td>NOM</td>\n",
       "      <td>there_is</td>\n",
       "      <td>there_</td>\n",
       "      <td>_is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>thought</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_thought</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>thought</td>\n",
       "      <td>she</td>\n",
       "      <td>was</td>\n",
       "      <td>NOM</td>\n",
       "      <td>thought_was</td>\n",
       "      <td>thought_</td>\n",
       "      <td>_was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>are</td>\n",
       "      <td>we</td>\n",
       "      <td>going</td>\n",
       "      <td>NOM</td>\n",
       "      <td>are_going</td>\n",
       "      <td>are_</td>\n",
       "      <td>_going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>he</td>\n",
       "      <td>fell</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_fell</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_fell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>can't</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_can't</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_can't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>they</td>\n",
       "      <td>are</td>\n",
       "      <td>NOM</td>\n",
       "      <td>&lt;s&gt;_are</td>\n",
       "      <td>&lt;s&gt;_</td>\n",
       "      <td>_are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>ask</td>\n",
       "      <td>him</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>ask_&lt;\\s&gt;</td>\n",
       "      <td>ask_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>in</td>\n",
       "      <td>them</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>in_&lt;\\s&gt;</td>\n",
       "      <td>in_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>in</td>\n",
       "      <td>them</td>\n",
       "      <td>then</td>\n",
       "      <td>ACC</td>\n",
       "      <td>in_then</td>\n",
       "      <td>in_</td>\n",
       "      <td>_then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>with</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>with_&lt;\\s&gt;</td>\n",
       "      <td>with_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>with</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>with_&lt;\\s&gt;</td>\n",
       "      <td>with_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>find</td>\n",
       "      <td>her</td>\n",
       "      <td>rattle</td>\n",
       "      <td>ACC</td>\n",
       "      <td>find_rattle</td>\n",
       "      <td>find_</td>\n",
       "      <td>_rattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>for</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>for_&lt;\\s&gt;</td>\n",
       "      <td>for_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>with</td>\n",
       "      <td>her</td>\n",
       "      <td>arm</td>\n",
       "      <td>ACC</td>\n",
       "      <td>with_arm</td>\n",
       "      <td>with_</td>\n",
       "      <td>_arm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>roll</td>\n",
       "      <td>her</td>\n",
       "      <td>back</td>\n",
       "      <td>ACC</td>\n",
       "      <td>roll_back</td>\n",
       "      <td>roll_</td>\n",
       "      <td>_back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>thank</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>thank_&lt;\\s&gt;</td>\n",
       "      <td>thank_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>here's</td>\n",
       "      <td>her</td>\n",
       "      <td>eyebrows</td>\n",
       "      <td>ACC</td>\n",
       "      <td>here's_eyebrows</td>\n",
       "      <td>here's_</td>\n",
       "      <td>_eyebrows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>want</td>\n",
       "      <td>me</td>\n",
       "      <td>to</td>\n",
       "      <td>ACC</td>\n",
       "      <td>want_to</td>\n",
       "      <td>want_</td>\n",
       "      <td>_to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>hand</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_hand</td>\n",
       "      <td>see_</td>\n",
       "      <td>_hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>let</td>\n",
       "      <td>me</td>\n",
       "      <td>see</td>\n",
       "      <td>ACC</td>\n",
       "      <td>let_see</td>\n",
       "      <td>let_</td>\n",
       "      <td>_see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>needs</td>\n",
       "      <td>her</td>\n",
       "      <td>rattle</td>\n",
       "      <td>ACC</td>\n",
       "      <td>needs_rattle</td>\n",
       "      <td>needs_</td>\n",
       "      <td>_rattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>rattle</td>\n",
       "      <td>her</td>\n",
       "      <td>pink</td>\n",
       "      <td>ACC</td>\n",
       "      <td>rattle_pink</td>\n",
       "      <td>rattle_</td>\n",
       "      <td>_pink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>maybe</td>\n",
       "      <td>her</td>\n",
       "      <td>key</td>\n",
       "      <td>ACC</td>\n",
       "      <td>maybe_key</td>\n",
       "      <td>maybe_</td>\n",
       "      <td>_key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>make</td>\n",
       "      <td>her</td>\n",
       "      <td>happy</td>\n",
       "      <td>ACC</td>\n",
       "      <td>make_happy</td>\n",
       "      <td>make_</td>\n",
       "      <td>_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>on</td>\n",
       "      <td>her</td>\n",
       "      <td>cushion</td>\n",
       "      <td>ACC</td>\n",
       "      <td>on_cushion</td>\n",
       "      <td>on_</td>\n",
       "      <td>_cushion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>draw</td>\n",
       "      <td>me</td>\n",
       "      <td>a</td>\n",
       "      <td>ACC</td>\n",
       "      <td>draw_a</td>\n",
       "      <td>draw_</td>\n",
       "      <td>_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>show</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>show_&lt;\\s&gt;</td>\n",
       "      <td>show_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>to</td>\n",
       "      <td>her</td>\n",
       "      <td>what</td>\n",
       "      <td>ACC</td>\n",
       "      <td>to_what</td>\n",
       "      <td>to_</td>\n",
       "      <td>_what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>to</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>to_&lt;\\s&gt;</td>\n",
       "      <td>to_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>want</td>\n",
       "      <td>me</td>\n",
       "      <td>to</td>\n",
       "      <td>ACC</td>\n",
       "      <td>want_to</td>\n",
       "      <td>want_</td>\n",
       "      <td>_to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>want</td>\n",
       "      <td>me</td>\n",
       "      <td>to</td>\n",
       "      <td>ACC</td>\n",
       "      <td>want_to</td>\n",
       "      <td>want_</td>\n",
       "      <td>_to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>hear</td>\n",
       "      <td>him</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>hear_&lt;\\s&gt;</td>\n",
       "      <td>hear_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>leave</td>\n",
       "      <td>her</td>\n",
       "      <td>here</td>\n",
       "      <td>ACC</td>\n",
       "      <td>leave_here</td>\n",
       "      <td>leave_</td>\n",
       "      <td>_here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>now</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_now</td>\n",
       "      <td>see_</td>\n",
       "      <td>_now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>now</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_now</td>\n",
       "      <td>see_</td>\n",
       "      <td>_now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>see</td>\n",
       "      <td>her</td>\n",
       "      <td>&lt;\\s&gt;</td>\n",
       "      <td>ACC</td>\n",
       "      <td>see_&lt;\\s&gt;</td>\n",
       "      <td>see_</td>\n",
       "      <td>_&lt;\\s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1748 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a     X         b case        predictor        aX         Xb\n",
       "0         oh     i      took  NOM          oh_took       oh_      _took\n",
       "1        <s>     i     think  NOM        <s>_think      <s>_     _think\n",
       "2        yes    he      gave  NOM         yes_gave      yes_      _gave\n",
       "3        <s>     i      just  NOM         <s>_just      <s>_      _just\n",
       "4    thought   she     might  NOM    thought_might  thought_     _might\n",
       "5         no     i     don't  NOM         no_don't       no_     _don't\n",
       "6        <s>     i     don't  NOM        <s>_don't      <s>_     _don't\n",
       "7      think   she     would  NOM      think_would    think_     _would\n",
       "8         if    we       can  NOM           if_can       if_       _can\n",
       "9        <s>    he   doesn't  NOM      <s>_doesn't      <s>_   _doesn't\n",
       "10     would    he      like  NOM       would_like    would_      _like\n",
       "11       <s>    he     would  NOM        <s>_would      <s>_     _would\n",
       "12        if    he     would  NOM         if_would       if_     _would\n",
       "13       <s>    he   doesn't  NOM      <s>_doesn't      <s>_   _doesn't\n",
       "14       <s>    he    wanted  NOM       <s>_wanted      <s>_    _wanted\n",
       "15       <s>    he       did  NOM          <s>_did      <s>_       _did\n",
       "16       <s>     i     don't  NOM        <s>_don't      <s>_     _don't\n",
       "17        do    we   have_to  NOM       do_have_to       do_   _have_to\n",
       "18      does   she      like  NOM        does_like     does_      _like\n",
       "19      well    we     can't  NOM       well_can't     well_     _can't\n",
       "20        if    we       can  NOM           if_can       if_       _can\n",
       "21        if     i       can  NOM           if_can       if_       _can\n",
       "22    pencil     i      know  NOM      pencil_know   pencil_      _know\n",
       "23     there   she        is  NOM         there_is    there_        _is\n",
       "24       <s>     i   thought  NOM      <s>_thought      <s>_   _thought\n",
       "25   thought   she       was  NOM      thought_was  thought_       _was\n",
       "26       are    we     going  NOM        are_going      are_     _going\n",
       "27       <s>    he      fell  NOM         <s>_fell      <s>_      _fell\n",
       "28       <s>     i     can't  NOM        <s>_can't      <s>_     _can't\n",
       "29       <s>  they       are  NOM          <s>_are      <s>_       _are\n",
       "..       ...   ...       ...  ...              ...       ...        ...\n",
       "497      ask   him      <\\s>  ACC         ask_<\\s>      ask_      _<\\s>\n",
       "498       in  them      <\\s>  ACC          in_<\\s>       in_      _<\\s>\n",
       "499       in  them      then  ACC          in_then       in_      _then\n",
       "500     with    me      <\\s>  ACC        with_<\\s>     with_      _<\\s>\n",
       "501     with    me      <\\s>  ACC        with_<\\s>     with_      _<\\s>\n",
       "502     find   her    rattle  ACC      find_rattle     find_    _rattle\n",
       "503      for   her      <\\s>  ACC         for_<\\s>      for_      _<\\s>\n",
       "504     with   her       arm  ACC         with_arm     with_       _arm\n",
       "505     roll   her      back  ACC        roll_back     roll_      _back\n",
       "506    thank   her      <\\s>  ACC       thank_<\\s>    thank_      _<\\s>\n",
       "507   here's   her  eyebrows  ACC  here's_eyebrows   here's_  _eyebrows\n",
       "508     want    me        to  ACC          want_to     want_        _to\n",
       "509      see   her      hand  ACC         see_hand      see_      _hand\n",
       "510      let    me       see  ACC          let_see      let_       _see\n",
       "511    needs   her    rattle  ACC     needs_rattle    needs_    _rattle\n",
       "512   rattle   her      pink  ACC      rattle_pink   rattle_      _pink\n",
       "513    maybe   her       key  ACC        maybe_key    maybe_       _key\n",
       "514     make   her     happy  ACC       make_happy     make_     _happy\n",
       "515       on   her   cushion  ACC       on_cushion       on_   _cushion\n",
       "516     draw    me         a  ACC           draw_a     draw_         _a\n",
       "517     show   her      <\\s>  ACC        show_<\\s>     show_      _<\\s>\n",
       "518       to   her      what  ACC          to_what       to_      _what\n",
       "519       to    me      <\\s>  ACC          to_<\\s>       to_      _<\\s>\n",
       "520     want    me        to  ACC          want_to     want_        _to\n",
       "521     want    me        to  ACC          want_to     want_        _to\n",
       "522     hear   him      <\\s>  ACC        hear_<\\s>     hear_      _<\\s>\n",
       "523    leave   her      here  ACC       leave_here    leave_      _here\n",
       "524      see   her       now  ACC          see_now      see_       _now\n",
       "525      see   her       now  ACC          see_now      see_       _now\n",
       "526      see   her      <\\s>  ACC         see_<\\s>      see_      _<\\s>\n",
       "\n",
       "[1748 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnn_aXb = dfnn_aXb_Nom.append(dfnn_aXb_Acc)\n",
    "dfnn_aXb['aX'] = dfnn_aXb['a']+'_'\n",
    "dfnn_aXb['Xb'] = '_' + dfnn_aXb['b']\n",
    "\n",
    "dfnn_aXb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfnn_aXb = shuffle(dfnn_aXb)\n",
    "dfnn_Eve = shuffle(dfnn_Eve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1398\n",
      "Test size: 350\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dfnn_aXb) * 0.8) \n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(dfnn_aXb) - train_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on aX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258 samples, validate on 140 samples\n",
      "Epoch 1/3\n",
      "1258/1258 [==============================] - 2s 2ms/step - loss: 0.6326 - acc: 0.6892 - val_loss: 0.5160 - val_acc: 0.7929\n",
      "Epoch 2/3\n",
      "1258/1258 [==============================] - 0s 344us/step - loss: 0.4939 - acc: 0.7067 - val_loss: 0.3643 - val_acc: 0.8214\n",
      "Epoch 3/3\n",
      "1258/1258 [==============================] - 0s 347us/step - loss: 0.3672 - acc: 0.8259 - val_loss: 0.2672 - val_acc: 0.9143\n"
     ]
    }
   ],
   "source": [
    "train_word = dfnn_aXb['aX'][:train_size]\n",
    "train_case = dfnn_aXb['case'][:train_size]\n",
    "test_word = dfnn_aXb['aX'][train_size:]\n",
    "test_case = dfnn_aXb['case'][train_size:]\n",
    "\n",
    "num_cases = 2\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, char_level=False)\n",
    "tokenizer.fit_on_texts(train_word)\n",
    "x_train = tokenizer.texts_to_matrix(train_word)\n",
    "x_test = tokenizer.texts_to_matrix(test_word)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_case)\n",
    "y_train = encoder.transform(train_case)\n",
    "y_test = encoder.transform(test_case)\n",
    "\n",
    "epochs = 3\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "number_hidden_units = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_hidden_units, input_shape=(1000,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))   # this is the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 77us/step\n",
      "aX-Test accuracy: 0.8971428578240531\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('aX-Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test on Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258 samples, validate on 140 samples\n",
      "Epoch 1/3\n",
      "1258/1258 [==============================] - 2s 2ms/step - loss: 0.6320 - acc: 0.7019 - val_loss: 0.5214 - val_acc: 0.7929\n",
      "Epoch 2/3\n",
      "1258/1258 [==============================] - 0s 303us/step - loss: 0.4962 - acc: 0.7178 - val_loss: 0.3655 - val_acc: 0.8571\n",
      "Epoch 3/3\n",
      "1258/1258 [==============================] - 0s 312us/step - loss: 0.3636 - acc: 0.8331 - val_loss: 0.2660 - val_acc: 0.9214\n"
     ]
    }
   ],
   "source": [
    "train_word = dfnn_aXb['aX'][:train_size]\n",
    "train_case = dfnn_aXb['case'][:train_size]\n",
    "test_word = dfnn_Eve['aX'][train_size:]\n",
    "test_case = dfnn_Eve['case'][train_size:]\n",
    "\n",
    "num_cases = 2\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, char_level=False)\n",
    "tokenizer.fit_on_texts(train_word)\n",
    "x_train = tokenizer.texts_to_matrix(train_word)\n",
    "x_test = tokenizer.texts_to_matrix(test_word)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_case)\n",
    "y_train = encoder.transform(train_case)\n",
    "y_test = encoder.transform(test_case)\n",
    "\n",
    "epochs = 3\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "number_hidden_units = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_hidden_units, input_shape=(1000,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))   # this is the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716/716 [==============================] - 0s 68us/step\n",
      "Eve,aX-Test accuracy: 0.9092178774279589\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Eve,aX-Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258 samples, validate on 140 samples\n",
      "Epoch 1/3\n",
      "1258/1258 [==============================] - 2s 2ms/step - loss: 0.6371 - acc: 0.7130 - val_loss: 0.5423 - val_acc: 0.7929\n",
      "Epoch 2/3\n",
      "1258/1258 [==============================] - 0s 342us/step - loss: 0.5272 - acc: 0.7250 - val_loss: 0.4223 - val_acc: 0.8357\n",
      "Epoch 3/3\n",
      "1258/1258 [==============================] - 0s 318us/step - loss: 0.4297 - acc: 0.7949 - val_loss: 0.3486 - val_acc: 0.8643\n"
     ]
    }
   ],
   "source": [
    "train_word = dfnn_aXb['Xb'][:train_size]\n",
    "train_case = dfnn_aXb['case'][:train_size]\n",
    "test_word = dfnn_aXb['Xb'][train_size:]\n",
    "test_case = dfnn_aXb['case'][train_size:]\n",
    "\n",
    "num_cases = 2\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, char_level=False)\n",
    "tokenizer.fit_on_texts(train_word)\n",
    "x_train = tokenizer.texts_to_matrix(train_word)\n",
    "x_test = tokenizer.texts_to_matrix(test_word)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_case)\n",
    "y_train = encoder.transform(train_case)\n",
    "y_test = encoder.transform(test_case)\n",
    "\n",
    "epochs = 3\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "number_hidden_units = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_hidden_units, input_shape=(1000,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))   # this is the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 86us/step\n",
      "Xb-Test accuracy: 0.8085714275496346\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Xb-Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test on Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258 samples, validate on 140 samples\n",
      "Epoch 1/3\n",
      "1258/1258 [==============================] - 2s 2ms/step - loss: 0.6397 - acc: 0.6812 - val_loss: 0.5436 - val_acc: 0.7857\n",
      "Epoch 2/3\n",
      "1258/1258 [==============================] - 0s 349us/step - loss: 0.5274 - acc: 0.7059 - val_loss: 0.4189 - val_acc: 0.8000\n",
      "Epoch 3/3\n",
      "1258/1258 [==============================] - 0s 383us/step - loss: 0.4302 - acc: 0.7893 - val_loss: 0.3485 - val_acc: 0.8571\n"
     ]
    }
   ],
   "source": [
    "train_word = dfnn_aXb['Xb'][:train_size]\n",
    "train_case = dfnn_aXb['case'][:train_size]\n",
    "test_word = dfnn_Eve['Xb'][train_size:]\n",
    "test_case = dfnn_Eve['case'][train_size:]\n",
    "\n",
    "num_cases = 2\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, char_level=False)\n",
    "tokenizer.fit_on_texts(train_word)\n",
    "x_train = tokenizer.texts_to_matrix(train_word)\n",
    "x_test = tokenizer.texts_to_matrix(test_word)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_case)\n",
    "y_train = encoder.transform(train_case)\n",
    "y_test = encoder.transform(test_case)\n",
    "\n",
    "epochs = 3\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "number_hidden_units = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_hidden_units, input_shape=(1000,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))   # this is the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716/716 [==============================] - 0s 73us/step\n",
      "Eve Xb-Test accuracy: 0.8770949717340523\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Eve Xb-Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on aXb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1258 samples, validate on 140 samples\n",
      "Epoch 1/3\n",
      "1258/1258 [==============================] - 2s 2ms/step - loss: 0.6265 - acc: 0.7051 - val_loss: 0.5033 - val_acc: 0.8214\n",
      "Epoch 2/3\n",
      "1258/1258 [==============================] - 0s 344us/step - loss: 0.4885 - acc: 0.7687 - val_loss: 0.3700 - val_acc: 0.8714\n",
      "Epoch 3/3\n",
      "1258/1258 [==============================] - 0s 357us/step - loss: 0.3622 - acc: 0.8553 - val_loss: 0.3073 - val_acc: 0.8929\n"
     ]
    }
   ],
   "source": [
    "train_word = dfnn_aXb['predictor'][:train_size]\n",
    "train_case = dfnn_aXb['case'][:train_size]\n",
    "test_word = dfnn_aXb['predictor'][train_size:]\n",
    "test_case = dfnn_aXb['case'][train_size:]\n",
    "\n",
    "num_cases = 2\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, char_level=False)\n",
    "tokenizer.fit_on_texts(train_word)\n",
    "x_train = tokenizer.texts_to_matrix(train_word)\n",
    "x_test = tokenizer.texts_to_matrix(test_word)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_case)\n",
    "y_train = encoder.transform(train_case)\n",
    "y_test = encoder.transform(test_case)\n",
    "\n",
    "epochs = 3\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "number_hidden_units = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_hidden_units, input_shape=(1000,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))   # this is the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 66us/step\n",
      "aXb-Test accuracy: 0.8942857125827245\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('aXb-Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train on Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1573 samples, validate on 175 samples\n",
      "Epoch 1/3\n",
      "1573/1573 [==============================] - 2s 1ms/step - loss: 0.6059 - acc: 0.7025 - val_loss: 0.5191 - val_acc: 0.7314\n",
      "Epoch 2/3\n",
      "1573/1573 [==============================] - 1s 336us/step - loss: 0.4359 - acc: 0.7947 - val_loss: 0.3890 - val_acc: 0.8400\n",
      "Epoch 3/3\n",
      "1573/1573 [==============================] - 1s 331us/step - loss: 0.3018 - acc: 0.8932 - val_loss: 0.2865 - val_acc: 0.8686\n"
     ]
    }
   ],
   "source": [
    "train_word = dfnn_aXb['predictor']\n",
    "train_case = dfnn_aXb['case']\n",
    "test_word = dfnn_Eve['predictor']\n",
    "test_case = dfnn_Eve['case']\n",
    "\n",
    "num_cases = 2\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, char_level=False)\n",
    "tokenizer.fit_on_texts(train_word)\n",
    "x_train = tokenizer.texts_to_matrix(train_word)\n",
    "x_test = tokenizer.texts_to_matrix(test_word)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_case)\n",
    "y_train = encoder.transform(train_case)\n",
    "y_test = encoder.transform(test_case)\n",
    "\n",
    "epochs = 3\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "number_hidden_units = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_hidden_units, input_shape=(1000,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))   # this is the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2114/2114 [==============================] - 0s 63us/step\n",
      "Eve aXb-Test accuracy: 0.7682119205298014\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Eve aXb-Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics \n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on aX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = dfnn_aXb['aX']\n",
    "labels = dfnn_aXb['case']\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(texts, labels)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aX, NB, WordLevel TF-IDF:  0.9633867276887872\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"aX, NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aX, RF, WordLevel TF-IDF:  0.9610983981693364\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"aX, RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test on Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = dfnn_aXb['aX']\n",
    "labels = dfnn_aXb['case']\n",
    "train_x = texts\n",
    "valid_x = dfnn_Eve['aX']\n",
    "train_y = labels\n",
    "valid_y = dfnn_Eve['case']\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eve, aX, NB, WordLevel TF-IDF:  0.9351939451277199\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Eve, aX, NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eve, aX, RF, WordLevel TF-IDF:  0.9351939451277199\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Eve, aX, RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = dfnn_aXb['Xb']\n",
    "labels = dfnn_aXb['case']\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(texts, labels)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xb, NB, WordLevel TF-IDF:  0.9016018306636155\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Xb, NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xb, RF, WordLevel TF-IDF:  0.897025171624714\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Xb, RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test on Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = dfnn_aXb['Xb']\n",
    "labels = dfnn_aXb['case']\n",
    "train_x = texts\n",
    "valid_x = dfnn_Eve['Xb']\n",
    "train_y = labels\n",
    "valid_y = dfnn_Eve['case']\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eve, Xb, NB, WordLevel TF-IDF:  0.8836329233680227\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Eve, Xb, NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eve, Xb, RF, WordLevel TF-IDF:  0.8878902554399243\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Eve, Xb, RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on aXb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = dfnn_aXb['predictor']\n",
    "labels = dfnn_aXb['case']\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(texts, labels)\n",
    "\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aXb, NB, WordLevel TF-IDF:  0.8695652173913043\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"aXb, NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aXb, RF, WordLevel TF-IDF:  0.8672768878718535\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"aXb, RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test on Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = dfnn_aXb['predictor']\n",
    "labels = dfnn_aXb['case']\n",
    "train_x = texts\n",
    "valid_x = dfnn_Eve['predictor']\n",
    "train_y = labels\n",
    "valid_y = dfnn_Eve['case']\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aXb, NB, WordLevel TF-IDF:  0.9016083254493851\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"aXb, NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aXb, RF, WordLevel TF-IDF:  0.902554399243141\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"aXb, RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
